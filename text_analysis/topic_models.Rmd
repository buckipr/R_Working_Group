---
title: "Topic Models with <br/><br/> ![](img/Rlogo.png){width=200px}"
author: "Jason Thomas"
institute: "R Working Group"
date: "March 7th, 2025"
output:
  xaringan::moon_reader:
    css: ["style_text_analysis.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      navigation:
        scroll: false
---

```{r setup, include=FALSE}
library(knitr)
library(emoji)
library(dplyr)
library(rvest)
library(tidyverse)
library(topicmodels)
library(tm)
library(SnowballC)
library(slam)
knitr::opts_chunk$set(tidy=FALSE, prompt=FALSE, error=TRUE, cache = TRUE)
```


# Introduction

* In this session, the second of a two-part series, we will
explore the structure in a **corpus** (or collection of documents).

--
  
* The "structure" is assumed by the choice of **topic model**:

--

  + **Latent Dirichlet Allocation model**  (LDA)

--

  + The corpus has a fixed number of *topics* (chosen *a priori*
  by the modeler)

--

  + Each topic has a probability for generating/producing each
  term in the vocabulary (i.e., all terms appearing in the
  corpus)

--

  + Each document is made up of a mixuture of topics (so its words
  are generated from the topics)


---
# Take 2

Yikes, that was quite the word salad!  Let's try again, but with pictures
from the expert.

--

* David M. Blei (Professor, Columbia University) has pioneered much of
  the work on LDA, and his [website](https://www.cs.columbia.edu/~blei/topicmodeling.html) includes
  useful resources for learning about many things, including topic models.
  
  + (on the stats-ier side of things)

--

* **The following two slides (19 & 20) are taken from Prof. Blei's 2012 ICML
presentation on topic models (obtained from his [website](https://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf))**


---
class: inverse, center, middle
background-image: url(img/blei_slide19_ICML_2012.png)
background-size: contain

<p style="color:red; font-weight:bold; font-size:32px; position: absolute; top: 40px">
<a href="https://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf">Source: David Blei's 2012 ICML presentation (slide 19)</a></p>

---
class: inverse, center, middle
background-image: url(img/blei_slide20_ICML_2012.png)
background-size: contain

<p style="color:red; font-weight:bold; font-size:32px; position: absolute; top: 40px">
<a href="https://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf">Source: David Blei's 2012 ICML presentation (slide 20)</a></p>


---
# More on Topic Models

* **LDA** is Latent Diri-what-now?

--

  + Dirichlet (a German mathematician) had a probability distribution
  named after him

--

  + The Dirichlet distribution is useful for generating probabilities
  (values between 0 and 1) for multiple categies
  
  + (think probabilities of a multinomial distribution)

--

* There are different types of topic models, some of which extend
the LDA (see Blei's presentation for detailed examples  )

--
  
  + (e.g, correlated topics, dynamic topic models, supervised LDA)

---
# Setup

We will use a few packages to make life easier...

```{r eval=FALSE}
# you can install the packages with the following command (only need to run once)
install.packages(c("topicmodel", "tm", "SnowballC", "slam"))
```

and load them with

```{r, eval=FALSE}
library(topicmodels)
library(tm)
library(SnowballC)
library(slam)
```


---
class: inverse, center, middle

# Example Data & Prep


---
# Example data

* We are going to follow the example in the [vignette](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf) from the R package [**topicmodels**](https://cran.r-project.org/package=topicmodels)

  + Abstracts from articles in the *Journal of Statistical Software*

--

```{r, collapse=TRUE}
data(JSS_papers)  ## load example dataset from topicmodels
colnames(JSS_papers)
```
--

The "description" contains the abstracts.  We'll use LDA to find topics that generate
the words in the abstract.


---
# Example data (cont)


```{css, echo=FALSE}
/* R code */
/*.foobar code.r {
  font-weight: bold;
}*/
/* code without language names */
.wrap code[class="remark-code"] {
  /*display: block;*/
  /*border: 1px solid red;*/
  text-wrap: wrap;
}
```


```{r}
JSS_papers[1, "title"]  ## title for 1st article
```


```{r}
JSS_papers[1, "creator"]  ## author(s) or 1st article
```

.wrap[
```{r, split=TRUE}
JSS_papers[1, "description"]  ## abstract for 1st article
```
]


---
# Data Prep

* We need to prepare our data by creating a document-term-matrix (as discussed in the
previous session) using tools from the [**tm** package](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf)

--

* First we create a corpus...

.wrap[
```{r}
corpus <- Corpus(VectorSource(unlist(JSS_papers[, "description"])))
inspect(corpus[1])
```
]


---
# Document-Term-Matrix

Now we can convert the corpus into a DTM (with some of preprocessing )

```{r}
jss_dtm <- DocumentTermMatrix(corpus,
                              control=list(tolower = TRUE,
                                           stemming = TRUE,
                                           stopwords = TRUE,
                                           minWordLength = 3,
                                           removeNumbers = TRUE,
                                           removePunctuation = TRUE))
jss_dtm
```


---
# (DTM Pit Stop)

* As you may have noticed, our DTM is pretty big and has lots of zeros!

* A special structure is used to efficiently work with large, sparse matrices

* **tm** provides useful tools for exploring your DTM

  + `nDocs(dtm)` -- number of documents (or rows) in your corpus/dtm
  
  + `nTerms(dtm)` -- number of terms (or columns) in your corpus/dtm
  
  + `inspect(dtm)` -- used for looking at slices (rows & columns) of your dtm


---
# Inspecting DTMs

(back to our regularly scheduled programming)

```{r}
nDocs(jss_dtm)
nTerms(jss_dtm)
```


---
# Inspecting DTMs (cont)

```{r}
inspect(jss_dtm[1:5, 1:5])
```


---
# TF-IDF

mean tf-idf (term frequency-inverse document frequency)

```{r}
term_tfidf <- tapply(jss_dtm$v/row_sums(jss_dtm)[jss_dtm$i],
                     jss_dtm$j, mean) *
  log2(nDocs(jss_dtm)/col_sums(jss_dtm > 0))
length(term_tfidf)
summary(term_tfidf)
jss_dtm <- jss_dtm[, term_tfidf >= 0.1]
dim(jss_dtm)
```



---
class: inverse, center, middle

# LDA Model


---
# LDA intro

There are different versions of the model available and different methods of 
parameter estimation estimation.

--

* LDA
  + VEM & Gibbs
  
* Correlated Topic Model with Gibbs


---
# LDA example


```{r}
# fit models
m1_k30 <- LDA(jss_dtm, k = 30)
m1_k30             ## S4 Object
slotNames(m1_k30)          ## can also access with m1_k30@alpha
slot(m1_k30, "alpha")      ## parameter for topic dist for docs
dim(slot(m1_k30, "beta"))  ## term distribution for each topic
```


---
# LDA example (cont)

Accessing results

```{r}
m1_k30_results <- posterior(m1_k30)
str(m1_k30_results)
names(m1_k30_results)
```


---
# LDA example (cont)

Pr(Term | Topic) -- term distribution over topics

```{r}
dim(posterior(m1_k30)$terms)
posterior(m1_k30)$terms[1,1:10]
```


---
# LDA example (cont)

Pr(Topic | Doc) -- topic distribution over documents


```{r}
dim(posterior(m1_k30)$topics)
posterior(m1_k30)$topics[1:3,]   
```


---
# LDA example (cont)

Most likely topic for each document

```{r}
topics(m1_k30, 1) 
```


---
# LDA example (cont)

5 most frequent terms for each topic

```{r}
terms(m1_k30, 5) 
```


---
# LDA example (cont)

20 most frequent terms for Topic 23

```{r}
terms(m1_k30, 20)[,"Topic 23"]
```


---
# LDA example (cont)


```{r}
m2_k30 <- LDA(jss_dtm, k = 30, method = "Gibbs")
```

---
# Other resources

* The **Text Mining with R** book/site has a few more 
[example applications of LDA](https://www.tidytextmining.com/topicmodeling)


