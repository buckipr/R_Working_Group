---
title: Introduction to <br><br> Regression in ![](img/Rlogo.png){width=120px}
author: "<br><br><br>Jason Thomas <br> <br>R Working Group"
date: "<br>November 8th, 2024"
output:
  xaringan::moon_reader:
    css: ["style_regression.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      navigation:
        scroll: false
---


```{r setup, include=FALSE}
library(knitr)
library(emoji)
library(tidyverse)
library(DT)
library(flextable)
library(gtsummary)
library(stargazer)
library(furniture)
library(prediction)
library(lm.beta)
library(margins)
library(dotwhisker)
library(MASS)
library(nnet)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(tidy=FALSE, prompt=FALSE, error=TRUE,
                      fig.align = 'center')
data(mtcars)
```


class: slide-font-25
# Introduction

This workshop covers steps for fitting regression models in
<img src="img/Rlogo.png" width="64"> and preparing tables and figures.

* Our workhorse will be the `lm()` function (and a few cousins, like `glm`)

* The basic recipe includes a *formula* for your model

  + `y ~ x1 + x2 + x3*x4`
  <br><br> (dependent variable `y` regressed on `x`'s)
  
\begin{eqnarray}
  \hat{y} &=& \alpha + \beta_1 x_1 + \beta_2 x_2 + \\ 
          && \;\;\; \beta_3 x_3 + \beta_4 x_4 + \beta_5 (x_3 \, x_4)
\end{eqnarray}
  
* Instead of `y` and `x`, you use the names of the variables in your data frame


---
# Introduction

* We can fit **generalized linear models** with `glm()` and a few more options

  + *family* will indicate the type of outcome (e.g., binomial or count),
  + *link* is an additional option that allows you to choose between members
  of the same family (e.g., logit vs. probit)
  + (more on this later)

* Yes, chef, but what is the recipe for...
  
  + Post-processing results? no problem
  + Figures? no problem
  + Tables? work in progress


---
# Introduction

* There has been significant progress in the tools for transfering results
to tables

  + depends on output format (e.g., Word/Excel vs. LaTeX)

* The main sticking point is with regression models

  + still need a tool for linking with Word/Excel
  + though we can build our own...


---
# Plan of attack

* Present our **descriptive statistics**

* **Linear regression**

  + models, tables, & plots

* Modeling **categorical outcomes**

* What to do with **count data**

* note: some of the tables/output may appear to run off the
end of the slide, but you should be able to scroll down and see
everything


---

We need a few R packages which can be installed with the following command:

```{r, echo=TRUE, eval=FALSE}
install.packages(c("flextable", "gtsummary" "stargazer", "furniture",
                   "margins", "prediction", "lm.beta", "dotwhisker"))
```

and the packages can be loaded with:

```{r, eval=FALSE}
library(flextable)
library(gtsummary)
library(stargazer)
library(furniture)
library(prediction)
library(margins)
library(lm.beta)
library(dotwhisker)
library(MASS)
library(nnet)
```


---
class: slide-font-25
# Useful Resources

* [`flextable` reference](https://ardata-fr.github.io/flextable-book/index.html)
  + [`crosstable`](https://danchaltiel.github.io/crosstable/) -- works with flextable

* `kableExtra` is also a great tool for making HTML & LaTeX tables
  + [HTML vignette](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html)
  + [LaTeX vignette](http://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf)
  + (you can copy/paste HTML tables into Word)

* [`stargazer` vignette](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf)

* [`margins` vignette](https://www.rdocumentation.org/packages/margins/versions/0.3.23)

* [`dotwhisker` vignette](https://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html)


---
class: inverse, center, middle

# Descriptive Statistics


---
# Descriptive statistics

* There are several packages which will do some/most/all of the work for you

  + LaTeX & HTML  -- `stargazer`
  + Word -- `flextable`, `gtsummary`, `furniture`
  <br> (use R Markdown & Knit to Word doc)

* However, the best route may be to create the table yourself and then
convert to a flextable (for Word)

  + or `write.csv()` to CSV file (for Excel/Word)


---
# Descriptive stats: html & latex

`stargazer` example (LOTS of options!)


```{r, echo=TRUE, eval=FALSE, results='asis'}
stargazer(mtcars, title = "Table 1. XXX", type = "html", # or "latex"
          out = "descriptives.html",
          summary.stat = c("mean", "sd", "min", "max"),
          notes = paste("Number of observations =", nrow(mtcars)))
# open the file descriptives.html, then copy and paste to excel
```

(table on next slide)

---
# Descriptive stats: html & latex

```{r, echo=FALSE, eval=TRUE, results='asis'}
stargazer(mtcars, title = "Table 1. XXX", type = "html", out = "descriptives.html",
          dep.var.caption = "Variable",
          summary.stat = c("mean", "sd", "min", "max"),
          notes = paste("Number of observations =", nrow(mtcars)))
```


---
# Descriptive stats: Word

`flextable` & `gtsummary`

```{r, echo=TRUE, eval=FALSE, results='asis'}
mtcars |>
  tbl_summary(
    type = all_continuous() ~ "continuous",
      statistic = all_continuous() ~ "mean") |>
  as_flex_table()
```

(table on next slide)

---
# Descriptive stats: Word

```{r, echo=FALSE, eval=TRUE, results='asis'}
mtcars |>
  tbl_summary(
    type = all_continuous() ~ "continuous",
      statistic = all_continuous() ~ "{mean}") |>
  as_flex_table()
```


---
# Descriptive stats: Word

by group with multiple stats

```{r, echo=TRUE, eval=FALSE, results='asis'}
mtcars |>
  tbl_summary(
    by = vs,
    type = all_continuous() ~ "continuous2",
    statistic = all_continuous() ~ c("{mean} ({sd})")) |>
  as_flex_table()
```

(table on next slide)

---
# Descriptive stats: Word

```{r, echo=FALSE, eval=TRUE, results='asis'}
mtcars |>
  tbl_summary(
    by = vs,
    type = all_continuous() ~ "continuous2",
    statistic = all_continuous() ~ c("{mean} ({sd})")) |>
  as_flex_table()
```



---
# Descriptive stats: Word

`furniture`

```{r, echo=TRUE, eval=FALSE, results='asis'}
vNames <- c("Miles per gallon", "Number of cylinders",
            "Gross horsepower")
table1(mtcars, mpg, cyl, hp, var_names = vNames,
       splitby = ~am,
       caption = "**Means and SD for subset of variables**",
       type = c("simple", "condensed"),
       total= TRUE,
       output = "pandoc")
```

(table on next slide)

---
# Descriptive stats: Word

```{r, echo=FALSE, eval=TRUE, results='asis'}
vNames <- c("Miles per gallon", "Number of cylinders",
            "Gross horsepower")
table1(mtcars, mpg, cyl, hp, var_names = vNames,
       splitby = ~am,
       caption = "**Means and SD for subset of variables**",
       type = c("simple", "condensed"),
       total= TRUE,
       output = "markdown") # use markdown for slides
```


---
class: slide-font-25
# Exercises

* Build a table of descriptive statistics for the `mtcars` data set
  
* Include useful variable names for the rows (e.g., miles per gallon)

* In the first column include the mean (2 decimal places) with std deviation
  in the row below with parenthesis around it 

* In the second column include the number of missing observations (go ahead
and randomly assign NAs to make it more interesting)

* Make another version of this table but with groups of columns for
"low" horsepower (hp), "high" horsepower, and the the whole sample ("low" and
"high" together)

  + can you build a function to do this more generally?


---
class: inverse, center, middle

# Linear Regression


---
class: slide-font-25
# Linear regression

**Example data set** duncan.csv

* unit of analysis: occupations

* `prestige`: % of respondents who rated the occupation as "good" or "excellent"

* `educ`: % of workers in the occupation who had a HS degree

* `income`: % of workers in the occupation who earned more than $3,500

* `occ_type`: indicator for type of occupation
    + blue collar (bc)
    + professional and managerial (prof)
    + and white collar (wc)


---
# Linear regression
```{r, echo=TRUE, eval=TRUE}
# load and summarize the data
duncan <- read.csv("duncan.csv")
summary(duncan)
```

`occ_type` is a categorical variable that we will want to treat as a factor
or with indicator variables


---
# Linear regression

Preparing categorical predictors/independent variables

```{r, echo=TRUE, eval=TRUE}
table(duncan$occ_type)

# factor
duncan$occ <- factor(
  duncan$occ_type,
  levels = c("prof", "wc", "bc"), # first is reference group
  labels = c("professional", "white_collar", "blue_collar"))

# indicator variables (dummies)
duncan$prof <- ifelse(duncan$occ_type == "prof", 1, 0 )
duncan$wc <- ifelse(duncan$occ_type == "wc", 1, 0 )
duncan$bc <- ifelse(duncan$occ_type == "bc", 1, 0 )
```


---
# Linear regression

Fit the model and print the results

```{r, echo=TRUE, eval=TRUE}
model1 <- lm(prestige ~ income + educ + occ, data = duncan)
model1
```


---
# Linear regression

Even more results

```{r, echo=TRUE, eval=TRUE}
summary(model1)
```


---
# Linear regression

Regression objects (and their summaries!) are `lists` that hold a lot more...

```{r, echo=TRUE, eval=TRUE}
names(model1)
names(summary(model1))
```

The $R^2$ for model 1 is `r round(summary(model1)$r.squared, 3)` (and
lives in summary(model1)$r.squared).


---
# Linear regression

```{r, echo=TRUE, eval=TRUE, out.width="50%"}
plot(model1$fitted, duncan$prestige, xlab="fitted", ylab="observed")
abline(a=0, b=1, col = "red")
```


---
# Linear regression

```{r, echo=TRUE, eval=TRUE, out.width="50%"}
plot(duncan$income, duncan$prestige, xlab="income", ylab="prestige")
abline(coef = coef(model1), col = "red", lwd = 2)
```

---
# Exercises

* Why does the regression line look "off"? (the predicted values seem to be
systematically too too low at higher levels of income)

* Make the previous figure with `ggplot()` with the different occupation types
(professional, blue collar, white collar) color coded, and no uncertainty
around the regression line

* Add a loess() line to the previous plot (with a different color)


---
# Exercises

Can you make this plot from model 1 results?

```{r, echo=FALSE, eval=TRUE, out.width="50%"}
plot(duncan$income, duncan$prestige,
     main = "Uncertainty for Model 1\n(prof w/ mean education)",
     xlab = "income", ylab = "prestige", type = "n", bty="n")
library(mvtnorm)
betas <- rmvnorm(n = 500, mean=model1$coef,
                 sigma = summary(model1)$cov.unscaled*(summary(model1)$sigma^2))
alpha <- betas %*% c(1, 0, mean(duncan$educ), 0, 0)
for(i in 1:nrow(betas)) {
  abline(a = alpha[i], b = betas[i, 2], col = "grey")
}
abline(coef=coef(model1)*c(1,1,1,0,0) + c(mean(duncan$educ)*coef(model1)[3], 0, 0, 0, 0),
       col = "red", lwd = 3)
```
(hint: `mvtnorm` is your friend)


---
# Linear regression

We can build more complicated models by updating simpler (nested) models

```{r}
# recall: 
# model1 <- lm(prestige ~ income + educ + occ, data = duncan)

# add interaction to model 1
model2 <- update(model1, . ~ . + income*educ, #<<
                 data = duncan)
```


---
class: codefs-50
# Linear regression

```{r}
summary(model2)
```


---
class: codefs-50
# Linear regression

We can **transform** variables inside `lm()` *if* we use a function
<br> e.g., `log()` or `sqrt()`

```{r}
model3 <- lm(prestige ~ educ*occ + log(income), data = duncan)
summary(model3)
```


---
class: codefs-50
# Linear regression

For polynomial terms (e.g, squared, cubed) we need to create transformed
variable before calling `lm()`

```{r}
duncan$income_sq <- duncan$income^2
model4 <- lm(prestige ~ educ*occ + income_sq, data = duncan)
summary(model4)
```


---
# Linear regression: table

```{r, eval=FALSE, results="asis"}
cov_labels <- c("Education", "log(Income)", "Income squared",
                "Income", "Income * Education", "White Collar", 
                "Education * White Collar", "Blue Collar",
                "Education * Blue Collar", "Intercept")
stargazer(model1, model2, model3, model4, type = "html", out = "models.html",
          model.numbers = FALSE, dep.var.caption = "", dep.var.labels.include = FALSE,
          column.labels = c("Model 1", "Model 2", "Model 3", "Model 4"),
          star.cutoffs = c(0.05, 0.01, 0.001),
          table.layout = "-c-!t-s-n",
          covariate.labels = cov_labels,
          order = c(2, 6, 7, 1, 5, 3, 8, 4, 9, 10),
          digits = 2, keep.stat = c("rsq", "n"))
```

(table is on the next slide)

---
class: slide-font-20
# Linear regression: table

```{r, eval=TRUE, echo=FALSE, results="asis"}
cov_labels <- c("Education", "log(Income)", "Income^{2}",
                "Income", "Income * Education", "White Collar", 
                "Education * White Collar", "Blue Collar",
                "Education * Blue Collar", "Intercept")
stargazer(model1, model2, model3, model4,
          type = "html", out = "models.html",
          model.numbers = FALSE, dep.var.caption = "",
          dep.var.labels.include = FALSE,
          column.labels = c("Model 1", "Model 2", "Model 3", "Model 4"),
          star.cutoffs = c(0.05, 0.01, 0.001),
          table.layout = "-c-!t-s-n",
          covariate.labels = cov_labels,
          order = c(2, 6, 7, 1, 5, 3, 8, 4, 9, 10),
          digits = 2, keep.stat = c("rsq", "n"))
```

---
class: codefs-50
# Linear regression: table

example with flextable?
`r emoji("crying_cat_face")` `r emoji("crying_cat_face")`
`r emoji("crying_cat_face")`


---
class: codefs-50
# Linear regression

Another useful option will **subset** the data

```{r, echo=TRUE}
duncan$occ_bc_wc <- duncan$occ_type == "bc" | duncan$occ_type == "wc"
mod1_bc_wc <- lm(prestige ~ income + educ, data = duncan, 
              subset = occ_bc_wc) #<<
summary(mod1_bc_wc)
```


---
# Linear regression

**Standardized regression coefficients** can be obtained with the `lm.beta` package

```{r, echo=TRUE, eval=TRUE}
summary(lm.beta(model1))
```


---
# Linear regression: predictions

Predicted values of prestige ( $\hat{y}$ ) using the `predictions` package

```{r, echo=TRUE, eval=TRUE}
p1 <- prediction(model1,
  at = list(occ = c("professional", "white_collar", "blue_collar")))
summary(p1)
```


---
# Linear regression: predictions

Now as a plot

```{r, echo=TRUE, eval=FALSE}
# set up the input with the expected format/names
yhat1 <- summary(p1)[, 1:5]
names(yhat1) <- c('term', 'estimate', 'std.error', 'statistic', 'p.value')
yhat1$term <- c('Professional', 'White Collar', 'Blue Collar')

# now the plot
dwplot(yhat1) ## based on normal distribution!
```

(plot on the next slide)


---
# Linear regression: predictions

```{r, echo=FALSE, eval=TRUE}
yhat1 <- summary(p1)[, 1:5]
names(yhat1) <- c('term', 'estimate', 'std.error', 'statistic', 'p.value')
yhat1$term <- c('Professional', 'White Collar', 'Blue Collar')
dwplot(yhat1)
```


---
# Linear regression: predictions

We could code this up ourselves with the CIs based on t-distribution...


```{r, echo=TRUE, eval=TRUE}
summary(p1)
cbind(yhat1[, 2] - qt(0.975, df = 39) * yhat1[, 3],
      yhat1[, 2] + qt(0.975, df = 39) * yhat1[, 3])
```



---
# Marginal effects: AME

Average marginal effects

```{r, echo=TRUE, eval=TRUE}
marg2 <- margins(model2)
summary(marg2)
```


---
# Marginal effects: AME

Plotting the AME

```{r, echo=TRUE, eval=FALSE}
plot(marg2)

## use the following code to save the plot as a pdf file
## pdf('marg1_plot.pdf')
## plot(marg1)
## dev.off()

## alternative functions, to pdf(), include bmp(), jpeg(),
## png(), and tiff()
```

(plot on next slide)


---
# Marginal effects: AME

```{r, echo=FALSE, eval=TRUE}
plot(marg2)
```



---
# Linear regression

Model comparison

```{r, echo=TRUE, eval=TRUE}
anova(model1, model2)
```

```{r, echo=TRUE, eval=TRUE}
# information criterion
BIC(model1, model2) # or AIC(model1, model2) if you prefer
```

---
class: inverse, center, middle

# Categorical Outcomes


---
# GLM

* Generalized linear model and the `glm()` function provide tools for working
with categorical dependent variables (and counts)

* To choose a specific model, we specify the family and line:

```{r, eval=FALSE}
binomial(link = "logit")     # or link = "probit"
gaussian(link = "identity")  # linear regression lm()
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")  # or link = "cloglog
quasi(link = "identity", variance = "constant")
quasibinomial(link = "logit")
quasipoisson(link = "log")
```


---
# Logistic regression

```{r, echo=TRUE, eval=TRUE}
grad_school <- read.csv('grad_school.csv')
summary(grad_school)
```


---
# Logistic regression

```{r, echo=TRUE, eval=TRUE}
logit1 <- glm(admit ~ gre + gpa + factor(rank), data = grad_school,
              family = binomial(link = "logit")) #<<
summary(logit1)
```


---
# Logistic regression

confidence intervals

```{r, echo=TRUE, eval=TRUE}
# confint(logit1, level = 0.99)  # based on profile likelihood
confint.default(logit1, level = 0.99) # based on standard errors
```


---
# Logistic regression

Odds ratios

```{r, echo=TRUE, eval=TRUE}
exp(cbind(coef(logit1), confint.default(logit1, level = 0.95)))
```

---
# Logistic regression

predicted probabilities

```{r, echo=TRUE, eval=TRUE}
df_predict <- with(grad_school, 
                   data.frame(gre = mean(gre), gpa = mean(gpa), 
                              rank = factor(1:4)))
df_predict
```

```{r, echo=TRUE, eval=TRUE}
logit1_pred <- predict(logit1, newdata = df_predict, type = "response")
logit1_pred
```

---
# Logistic regression

Average marginal effects
```{r, echo=TRUE, eval=TRUE}
margins(logit1, type = "response")
```



---
# Probit model
```{r, echo=TRUE, eval=TRUE}
probit1 <- glm(admit ~ gre + gpa + factor(rank), data = grad_school,
              family = binomial(link = "probit")) #<<
summary(probit1)
```


---
# Probit model
```{r, echo=TRUE, eval=TRUE}
# change reference category
grad_school$f_rank <- factor(grad_school$rank)
levels(grad_school$f_rank)
grad_school$f_rank <- relevel(grad_school$f_rank, ref = "3")
table(grad_school$f_rank)
```


---
# Probit model
```{r, echo=TRUE, eval=TRUE}
probit2 <- glm(admit ~ gre + gpa + f_rank, data = grad_school,
              family = binomial(link = "probit"))
summary(probit2)
```


---
# Multinomial regression

```{r, echo=TRUE, eval=TRUE}
hsbdemo <- read.csv('hsbdemo.csv')
summary(hsbdemo)
```


---
# Multinomial regression
```{r, echo=TRUE, eval=TRUE}
mnlog1 <- multinom(factor(prog) ~ factor(ses) + write, data = hsbdemo)
summary(mnlog1)
```


---
class: inverse, center, middle

# Modeling Counts


---
# Poisson model

```{r, echo=TRUE, eval=TRUE}
school_absence <- read.csv('school_absence.csv')
summary(school_absence)
```


---
# Poisson model

```{r, echo=TRUE, eval=TRUE}
pois1 <- glm(daysabs ~ math + factor(prog), data = school_absence,
             family=poisson(link = "log"))  #<<
summary(pois1)
```


---
# Negative Binomial

```{r, echo=TRUE, eval=TRUE}
nbreg1 <- glm.nb(daysabs ~ math + factor(prog), data = school_absence)
summary(nbreg1)
```



---
# To Do

* still need to check out the 
[effects package](https://cran.r-project.org/web/packages/effects/index.html)?

