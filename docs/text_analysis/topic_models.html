<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Topic Models with    </title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Thomas" />
    <script src="libs/header-attrs-2.29/header-attrs.js"></script>
    <link rel="stylesheet" href="style_text_analysis.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Topic Models with <br/><br/> <img src="img/Rlogo.png" width="200" />
]
.author[
### Jason Thomas
]
.institute[
### R Working Group
]
.date[
### March 7th, 2025
]

---





# Introduction

* In this session, the second of a two-part series, we will
explore the structure in a **corpus** (or collection of documents).

--
  
* The "structure" is assumed by the choice of **topic model**:

--

  + **Latent Dirichlet Allocation model**  (LDA)

--

  + The corpus has a fixed number of *topics* (chosen *a priori*
  by the modeler)

--

  + Each topic has a probability for generating/producing each
  term in the vocabulary (i.e., all terms appearing in the
  corpus)

--

  + Each document is made up of a mixuture of topics (so its words
  are generated from the topics)


---
# Take 2

Yikes, that was quite the word salad!  Let's try again, but with pictures
from the expert.

--

* David M. Blei (Professor, Columbia University) has pioneered much of
  the work on LDA, and his [website](https://www.cs.columbia.edu/~blei/topicmodeling.html) includes
  useful resources for learning about many things, including topic models.
  
  + (on the stats-ier side of things)

--

* **The following two slides (19 &amp; 20) are taken from Prof. Blei's 2012 ICML
presentation on topic models (obtained from his [website](https://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf))**


---
class: inverse, center, middle
background-image: url(img/blei_slide19_ICML_2012.png)
background-size: contain

&lt;p style="color:red; font-weight:bold; font-size:32px; position: absolute; top: 40px"&gt;
&lt;a href="https://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf"&gt;Source: David Blei's 2012 ICML presentation (slide 19)&lt;/a&gt;&lt;/p&gt;

---
class: inverse, center, middle
background-image: url(img/blei_slide20_ICML_2012.png)
background-size: contain

&lt;p style="color:red; font-weight:bold; font-size:32px; position: absolute; top: 40px"&gt;
&lt;a href="https://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf"&gt;Source: David Blei's 2012 ICML presentation (slide 20)&lt;/a&gt;&lt;/p&gt;


---
# More on Topic Models

* **LDA** is Latent Diri-what-now?

--

  + Dirichlet (a German mathematician) had a probability distribution
  named after him

--

  + The Dirichlet distribution is useful for generating probabilities
  (values between 0 and 1) for multiple categies
  
  + (think probabilities of a multinomial distribution)

--

* There are different types of topic models, some of which extend
the LDA (see Blei's presentation for detailed examples  )

--
  
  + (e.g, correlated topics, dynamic topic models, supervised LDA)

---
# Setup

We will use a few packages to make life easier...


``` r
# you can install the packages with the following command (only need to run once)
install.packages(c("topicmodel", "tm", "SnowballC", "slam"))
```

and load them with


``` r
library(topicmodels)
library(tm)
library(SnowballC)
library(slam)
```


---
class: inverse, center, middle

# Example Data &amp; Prep


---
# Example data

* We are going to follow the example in the [vignette](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf) from the R package [**topicmodels**](https://cran.r-project.org/package=topicmodels)

  + Abstracts from articles in the *Journal of Statistical Software*

--


``` r
data(JSS_papers)  ## load example dataset from topicmodels
colnames(JSS_papers)
##  [1] "title"       "creator"     "subject"     "description" "publisher"  
##  [6] "contributor" "date"        "type"        "format"      "identifier" 
## [11] "source"      "language"    "relation"    "coverage"    "rights"
```
--

The "description" contains the abstracts.  We'll use LDA to find topics that generate
the words in the abstract.


---
# Example data (cont)


&lt;style type="text/css"&gt;
/* R code */
/*.foobar code.r {
  font-weight: bold;
}*/
/* code without language names */
.wrap code[class="remark-code"] {
  /*display: block;*/
  /*border: 1px solid red;*/
  text-wrap: wrap;
}
&lt;/style&gt;



``` r
JSS_papers[1, "title"]  ## title for 1st article
```

```
## $title
## [1] "A Diagnostic to Assess the Fit of a Variogram Model to Spatial Data"
```



``` r
JSS_papers[1, "creator"]  ## author(s) or 1st article
```

```
## $creator
## [1] "Barry, Ronald"
```

.wrap[

``` r
JSS_papers[1, "description"]  ## abstract for 1st article
```

```
## $description
## [1] "The fit of a variogram model to spatially-distributed data is often difficult to assess. A graphical diagnostic written in S-plus is introduced that allows the user to determine both the general quality of the fit of a variogram model, and to find specific pairs of locations that do not have measurements that are consonant with the fitted variogram. It can help identify nonstationarity, outliers, and poor variogram fit in general. Simulated data sets and a set of soil nitrogen concentration data are examined using this graphical diagnostic."
```
]


---
# Data Prep

* We need to prepare our data by creating a document-term-matrix (as discussed in the
previous session) using tools from the [**tm** package](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf)

--

* First we create a corpus...

.wrap[

``` r
corpus &lt;- Corpus(VectorSource(unlist(JSS_papers[, "description"])))
inspect(corpus[1])
```

```
## &lt;&lt;SimpleCorpus&gt;&gt;
## Metadata:  corpus specific: 1, document level (indexed): 0
## Content:  documents: 1
## 
## [1] The fit of a variogram model to spatially-distributed data is often difficult to assess. A graphical diagnostic written in S-plus is introduced that allows the user to determine both the general quality of the fit of a variogram model, and to find specific pairs of locations that do not have measurements that are consonant with the fitted variogram. It can help identify nonstationarity, outliers, and poor variogram fit in general. Simulated data sets and a set of soil nitrogen concentration data are examined using this graphical diagnostic.
```
]


---
# Document-Term-Matrix

Now we can convert the corpus into a DTM (with some of preprocessing )


``` r
jss_dtm &lt;- DocumentTermMatrix(corpus,
                              control=list(tolower = TRUE,
                                           stemming = TRUE,
                                           stopwords = TRUE,
                                           minWordLength = 3,
                                           removeNumbers = TRUE,
                                           removePunctuation = TRUE))
jss_dtm
```

```
## &lt;&lt;DocumentTermMatrix (documents: 361, terms: 3497)&gt;&gt;
## Non-/sparse entries: 19199/1243218
## Sparsity           : 98%
## Maximal term length: 36
## Weighting          : term frequency (tf)
```


---
# (DTM Pit Stop)

* As you may have noticed, our DTM is pretty big and has lots of zeros!

* A special structure is used to efficiently work with large, sparse matrices

* **tm** provides useful tools for exploring your DTM

  + `nDocs(dtm)` -- number of documents (or rows) in your corpus/dtm
  
  + `nTerms(dtm)` -- number of terms (or columns) in your corpus/dtm
  
  + `inspect(dtm)` -- used for looking at slices (rows &amp; columns) of your dtm


---
# Inspecting DTMs

(back to our regularly scheduled programming)


``` r
nDocs(jss_dtm)
```

```
## [1] 361
```

``` r
nTerms(jss_dtm)
```

```
## [1] 3497
```


---
# Inspecting DTMs (cont)


``` r
inspect(jss_dtm[1:5, 1:5])
```

```
## &lt;&lt;DocumentTermMatrix (documents: 5, terms: 5)&gt;&gt;
## Non-/sparse entries: 0/25
## Sparsity           : 100%
## Maximal term length: 9
## Weighting          : term frequency (tf)
## Sample             :
##     Terms
## Docs “formula” abc abil abl absenc
##    1         0   0    0   0      0
##    2         0   0    0   0      0
##    3         0   0    0   0      0
##    4         0   0    0   0      0
##    5         0   0    0   0      0
```


---
# TF-IDF

mean tf-idf (term frequency-inverse document frequency)


``` r
term_tfidf &lt;- tapply(jss_dtm$v/row_sums(jss_dtm)[jss_dtm$i],
                     jss_dtm$j, mean) *
  log2(nDocs(jss_dtm)/col_sums(jss_dtm &gt; 0))
length(term_tfidf)
```

```
## [1] 3497
```

``` r
summary(term_tfidf)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.01555 0.07614 0.10165 0.12944 0.14583 1.17184
```

``` r
jss_dtm &lt;- jss_dtm[, term_tfidf &gt;= 0.1]
dim(jss_dtm)
```

```
## [1]  361 1788
```



---
class: inverse, center, middle

# LDA Model


---
# LDA intro

There are different versions of the model available and different methods of 
parameter estimation estimation.

--

* LDA
  + VEM &amp; Gibbs
  
* Correlated Topic Model with Gibbs


---
# LDA example



``` r
# fit models
m1_k30 &lt;- LDA(jss_dtm, k = 30)
m1_k30             ## S4 Object
```

```
## A LDA_VEM topic model with 30 topics.
```

``` r
slotNames(m1_k30)          ## can also access with m1_k30@alpha
```

```
##  [1] "alpha"           "call"            "Dim"             "control"        
##  [5] "k"               "terms"           "documents"       "beta"           
##  [9] "gamma"           "wordassignments" "loglikelihood"   "iter"           
## [13] "logLiks"         "n"
```

``` r
slot(m1_k30, "alpha")      ## parameter for topic dist for docs
```

```
## [1] 0.009447135
```

``` r
dim(slot(m1_k30, "beta"))  ## term distribution for each topic
```

```
## [1]   30 1788
```


---
# LDA example (cont)

Accessing results


``` r
m1_k30_results &lt;- posterior(m1_k30)
str(m1_k30_results)
```

```
## List of 2
##  $ terms : num [1:30, 1:1788] 2.33e-260 4.00e-238 9.77e-260 1.24e-238 1.25e-259 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:30] "1" "2" "3" "4" ...
##   .. ..$ : chr [1:1788] "“formula”" "abc" "absorb" "abund" ...
##  $ topics: num [1:361, 1:30] 0.001319 0.001736 0.164573 0.762 0.000216 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:361] "1" "2" "3" "4" ...
##   .. ..$ : chr [1:30] "1" "2" "3" "4" ...
```

``` r
names(m1_k30_results)
```

```
## [1] "terms"  "topics"
```


---
# LDA example (cont)

Pr(Term | Topic) -- term distribution over topics


``` r
dim(posterior(m1_k30)$terms)
```

```
## [1]   30 1788
```

``` r
posterior(m1_k30)$terms[1,1:10]
```

```
##       “formula”             abc          absorb           abund          accept 
##   1.216339e-313   2.013054e-313    4.160713e-03   6.794316e-313   5.332809e-267 
## acceptandreject          access        accommod       accompani      accomplish 
##   8.830040e-314   6.073120e-267    4.160713e-03   4.888071e-315    8.321426e-03
```


---
# LDA example (cont)

Pr(Topic | Doc) -- topic distribution over documents



``` r
dim(posterior(m1_k30)$topics)
```

```
## [1] 361  30
```

``` r
posterior(m1_k30)$topics[1:3,]   
```

```
##              1            2            3            4            5            6
## 1 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968
## 2 0.0010176358 0.7179720160 0.0010176358 0.1261133256 0.0010176358 0.0010176358
## 3 0.0007690968 0.7214689559 0.0007690968 0.0007690968 0.0007690968 0.0007690968
##              7            8            9           10           11           12
## 1 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968
## 2 0.0010176358 0.0010176358 0.0010176358 0.0010176358 0.0010176358 0.0010176358
## 3 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0821796881 0.0007690968
##             13           14           15           16           17           18
## 1 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968
## 2 0.0010176358 0.0010176358 0.1284384912 0.0010176358 0.0010176358 0.0010176358
## 3 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968
##             19           20           21           22           23           24
## 1 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.9776961922 0.0007690968
## 2 0.0010176358 0.0010176358 0.0010176358 0.0010176358 0.0010176358 0.0010176358
## 3 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968
##             25           26           27           28           29           30
## 1 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968 0.0007690968
## 2 0.0010176358 0.0010176358 0.0010176358 0.0010176358 0.0010176358 0.0010176358
## 3 0.0007690968 0.1755857418 0.0007690968 0.0007690968 0.0007690968 0.0007690968
```


---
# LDA example (cont)

Most likely topic for each document


``` r
topics(m1_k30, 1) 
```

```
##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 
##  23   2   2  26  24  24  12   4  26  10   6  17   7  14  12  22  14  24  17   4 
##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 
##  19   6  20  17   3  29  29  23   2   6  16  24  20   9  23   2  29   3  29  25 
##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 
##  14  16  30   1  26  28  15   1  15   8  20   7  25  12  22  11   9  15  21  13 
##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 
##  28  24  19  16   5  10  20   2  21  17  15  13  23  23   2  11  17   6  24  12 
##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 
##  29   1  23  21   1  23   9   9   4  22   3  16  28  19  27  20  30  23   2  26 
## 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 
##  22  29  22   2  24  25  29  12  21  12  10  25  14  16   3   9  23  25  13  16 
## 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 
##   9   7  23  30  25  26  21   1  17  22  30  10  14   2   1  10   7  29  25  13 
## 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 
##  20   3  14  10  25   5  29  22  30   6   7  30  20   2   5  13  10  17   4   1 
## 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 
##  17   5  25  12  22  12  26  29  13  13  18  18   2  15  13  22   5  30  29  15 
## 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 
##   5  28   5  18  28  21  11  18  16  23   1  20  13   6   4  17  27  25   7  28 
## 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 
##  15  10  28  25   1  28  18  23  16   1   2   6   4  10  29  10  17  10  28  16 
## 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 
##  26  18   2   1  11  29  19   8  22  26   3   8  30  21  26  26  28  14   6   5 
## 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 
##   7   6   6  25  29   7  19  13  13  13  19  13  13  13  13  19  11  11  19  23 
## 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 
##  30   5  11   3  11  13  21  24  13  23  23   9  20  27  14  17  30  30  27   5 
## 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 
##  25  27  19   8  22  13  28   2   6   4   6   7  10  18   3  20  12  17  22   1 
## 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 
##   8   6   1  14   3   4  29   8  23   3  15  25  21  17  25   6  19  20  23  14 
## 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 
##  19  17  25  30  20  16   9   4  16  26   1   1  10   8   6  29  17  14   5  11 
## 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 
##  27  21  27  18  10   6  20   8  18  20   7  19  20  23   4  20   4  15  22   8 
## 361 
##   5
```


---
# LDA example (cont)

5 most frequent terms for each topic


``` r
terms(m1_k30, 5) 
```

```
##      Topic 1    Topic 2     Topic 3     Topic 4      Topic 5    Topic 6   
## [1,] "biplot"   "lispstat"  "threshold" "miss"       "item"     "copula"  
## [2,] "winbug"   "balanc"    "wavelet"   "imput"      "irt"      "document"
## [3,] "macro"    "lisp"      "sequenc"   "longitudin" "rasch"    "text"    
## [4,] "survey"   "brief"     "bay"       "mixtur"     "trait"    "pattern" 
## [5,] "polynomi" "treatment" "intend"    "cancer"     "haplotyp" "random"  
##      Topic 7   Topic 8     Topic 9       Topic 10   Topic 11   Topic 12 
## [1,] "command" "genet"     "subset"      "curv"     "formula"  "control"
## [2,] "gui"     "rubi"      "autoregress" "roc"      "recurs"   "format" 
## [3,] "gee"     "criterion" "tree"        "socr"     "cumul"    "sunflow"
## [4,] "polyk"   "rinrubi"   "perl"        "speci"    "accuraci" "imag"   
## [5,] "tumor"   "parallel"  "decis"       "hypothes" "digit"    "bin"    
##      Topic 13    Topic 14  Topic 15   Topic 16   Topic 17     Topic 18  
## [1,] "network"   "control" "matric"   "random"   "catastroph" "imag"    
## [2,] "graph"     "chart"   "distanc"  "captur"   "itemset"    "popul"   
## [3,] "ergm"      "beta"    "mantel"   "period"   "quantit"    "tsp"     
## [4,] "econometr" "train"   "bilinear" "popul"    "frequent"   "fluoresc"
## [5,] "volum"     "convex"  "orient"   "mathemat" "cobb"       "partit"  
##      Topic 19    Topic 20  Topic 21 Topic 22   Topic 23   Topic 24  Topic 25 
## [1,] "mixtur"    "cluster" "diffus" "densiti"  "dose"     "www"     "pattern"
## [2,] "posit"     "vector"  "record" "variat"   "smooth"   "java"    "map"    
## [3,] "posterior" "machin"  "seriat" "random"   "cell"     "cgi"     "logit"  
## [4,] "densiti"   "highord" "censor" "zeroinfl" "forecast" "server"  "ssa"    
## [5,] "cluster"   "logit"   "niss"   "event"    "surfac"   "browser" "excel"  
##      Topic 26   Topic 27     Topic 28     Topic 29    Topic 30    
## [1,] "cluster"  "gene"       "ecolog"     "confid"    "scale"     
## [2,] "vista"    "wave"       "multilevel" "xlispstat" "aspect"    
## [3,] "index"    "microarray" "polit"      "smooth"    "intern"    
## [4,] "lispstat" "ordin"      "aggreg"     "impos"     "sandwich"  
## [5,] "field"    "impur"      "bias"       "outlier"   "correspond"
```


---
# LDA example (cont)

20 most frequent terms for Topic 23


``` r
terms(m1_k30, 20)[,"Topic 23"]
```

```
##  [1] "dose"        "smooth"      "cell"        "forecast"    "surfac"     
##  [6] "doserespons" "benchmark"   "factori"     "fuzzi"       "grid"       
## [11] "habitat"     "lack"        "variogram"   "spline"      "control"    
## [16] "bclass"      "contour"     "count"       "entri"       "fet"
```


---
# LDA example (cont)



``` r
m2_k30 &lt;- LDA(jss_dtm, k = 30, method = "Gibbs")
```

---
# Other resources

* The **Text Mining with R** book/site has a few more 
[example applications of LDA](https://www.tidytextmining.com/topicmodeling)


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
